<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Anne Bernard">
<meta name="dcterms.date" content="2023-09-19">

<title>TP2 - arbres de décision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="TP2-arbres_de_decision_files/libs/clipboard/clipboard.min.js"></script>
<script src="TP2-arbres_de_decision_files/libs/quarto-html/quarto.js"></script>
<script src="TP2-arbres_de_decision_files/libs/quarto-html/popper.min.js"></script>
<script src="TP2-arbres_de_decision_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="TP2-arbres_de_decision_files/libs/quarto-html/anchor.min.js"></script>
<link href="TP2-arbres_de_decision_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="TP2-arbres_de_decision_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="TP2-arbres_de_decision_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="TP2-arbres_de_decision_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="TP2-arbres_de_decision_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">TP2 - arbres de décision</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Anne Bernard </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 19, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sommaire</h2>
   
  <ul>
  <li><a href="#arbre-de-décision---algorithme-cart" id="toc-arbre-de-décision---algorithme-cart" class="nav-link active" data-scroll-target="#arbre-de-décision---algorithme-cart">Arbre de décision - algorithme CART</a>
  <ul class="collapse">
  <li><a href="#classification-avec-les-arbres" id="toc-classification-avec-les-arbres" class="nav-link" data-scroll-target="#classification-avec-les-arbres">Classification avec les arbres</a>
  <ul class="collapse">
  <li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1">Question 1 </a><a name="Question 1" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2">Question 2 </a><a name="Question 2" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3">Question 3 </a><a name="Question 3" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4">Question 4 </a><a name="Question 4" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5">Question 5 </a><a name="Question 5" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6">Question 6 </a><a name="Question 6" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  </ul></li>
  <li><a href="#méthodes-de-choix-de-paramètres---sélection-de-modèle" id="toc-méthodes-de-choix-de-paramètres---sélection-de-modèle" class="nav-link" data-scroll-target="#méthodes-de-choix-de-paramètres---sélection-de-modèle">Méthodes de choix de paramètres - Sélection de modèle</a>
  <ul class="collapse">
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7">Question 7 </a><a name="Question 7" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  <li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8">Question 8 </a><a name="Question 8" class="nav-link" data-scroll-target="undefined" href=""></a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="arbre-de-décision---algorithme-cart" class="level1">
<h1>Arbre de décision - algorithme CART</h1>
<p><a name="Arbre de décision - algorithme CART" href=""></a></p>
<section id="classification-avec-les-arbres" class="level2">
<h2 class="anchored" data-anchor-id="classification-avec-les-arbres">Classification avec les arbres</h2>
<p><a name="Classification avec les arbres" href=""></a></p>
<p>Dans toute la suite, nous fixerons une graine pour pouvoir discuter des résultats plus simplement car c’est aléatoire donc nous pouvons ne pas obtenir exactement les mêmes résultats à chaque simulation.</p>
<section id="question-1" class="level3">
<h3 class="anchored" data-anchor-id="question-1">Question 1 <a name="Question 1" href=""></a></h3>
<p>Dans le cadre de la régression, lorsque l’on cherche à prédire une valeur numérique pour <span class="math inline">\(Y\)</span>, on peut utiliser la variance comme moyen de mesurer l’homogénéité. En effet, plus la variance entre les données est élevée plus les données sont hétérogènes, plus elle est faible et plus les données sont homogènes.</p>
</section>
<section id="question-2" class="level3">
<h3 class="anchored" data-anchor-id="question-2">Question 2 <a name="Question 2" href=""></a></h3>
<p>Dans un premier temps, créons deux arbres de décision avec les critères de classification suivants : l’indice de gini et l’entropie. Nous simulons ensuite avec <code>rand_checkers</code> un échantillon de taille <span class="math inline">\(n=456\)</span>.</p>
<p>On partitionne ensuite en 2 sous-ensembles pour avoir un ensemble d’entrainement et un ensemble de test.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 456 = 114*4</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rand_checkers(n1<span class="op">=</span><span class="dv">114</span>, n2<span class="op">=</span><span class="dv">114</span>, n3<span class="op">=</span><span class="dv">114</span>, n4<span class="op">=</span><span class="dv">114</span>, sigma<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> data[:, :<span class="dv">2</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data[:, <span class="dv">2</span>].astype(<span class="bu">int</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>dt_gini.fit(X_train, Y_train)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>dt_entropy.fit(X_train, Y_train)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"error with gini criterion :"</span>, <span class="dv">1</span><span class="op">-</span>dt_gini.score(X_train, Y_train))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"error with entropy criterion :"</span>, <span class="dv">1</span><span class="op">-</span>dt_entropy.score(X_train, Y_train))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>error with gini criterion : 0.0
error with entropy criterion : 0.0</code></pre>
</div>
</div>
<p>Nous avons regardé les erreurs pour l’indice de gini et l’entropie sur les données d’entrainements et nous obtenons 0 pour les deux. En effet, nous avons le même échantillon pour les variables à prédire que pour les observations, donc nous apprenons très bien sur cet échantillon. Nous pourrions regarder pour quelle profondeur nous obtenons <span class="math inline">\(0\)</span>. Probablement que l’algorithme s’arrête lorsque l’on atteint cette valeur (<span class="math inline">\(1\)</span> en l’occurence car il raisonne en terme de score).</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>Error with entropy criterion:  [0.72991071 0.68080357 0.60491071 0.52455357 0.39732143 0.29017857
 0.12723214 0.06696429 0.04464286 0.02232143 0.01116071 0.00223214]
Error with Gini criterion:  [0.73214286 0.70089286 0.62053571 0.47767857 0.33705357 0.19866071
 0.06473214 0.03794643 0.02455357 0.01339286 0.00892857 0.00223214]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-4-output-2.png" width="1128" height="771" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-4-output-3.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Lorsque l’on regarde l’erreur pour différentes profondeurs de l’arbre, on se rend compte que plus il est profond plus l’on se rapproche de <span class="math inline">\(0\)</span>. Ici nous avons pris max_depth=<span class="math inline">\(12\)</span>. En effet au-delà de cette valeur l’erreur est nulle ou presque. Nous pouvons le visualiser sur les courbes.</p>
<p>On peut noter également que si l’on prend un arbre avec une petite profondeur, l’erreur est très élevée car si les données sont bien réparties et que l’on ne peut faire qu’une coupe par exemple, nous aurons deux classes très hétérogènes.</p>
<p>Nous avons également afficher les frontières et nous pouvons observer une belle classification des données lorsque la profondeur est élevée.</p>
</section>
<section id="question-3" class="level3">
<h3 class="anchored" data-anchor-id="question-3">Question 3 <a name="Question 3" href=""></a></h3>
<p>Affichons la classification que l’on obtient avec la profondeur qui minimise le pourcentage d’erreurs obtenues avec l’entropie. Nous avons vu que pour depth=<span class="math inline">\(12\)</span> l’erreur est nulle (le score vaut <span class="math inline">\(1\)</span>, score=1-erreur).</p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-stdout">
<pre><code>Best scores with entropy criterion:  0.9977678571428571</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-5-output-2.png" width="438" height="417" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="question-4" class="level3">
<h3 class="anchored" data-anchor-id="question-4">Question 4 <a name="Question 4" href=""></a></h3>
<p>Grâce aux lignes suivantes, nous pouvons réaliser l’arbre de décision pour le critère d’entropie. Affichons l’arbre en question :</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>tree.plot_tree(dt_entropy)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(dt_entropy, out_file<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>graph.render(<span class="st">"./arbre/arbre"</span>, <span class="bu">format</span><span class="op">=</span><span class="st">'pdf'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="code/arbre/arbre.png" width="800"></p>
<p>Un arbre de décision est assez simple à lire. Tout d’abord il se compose d’un noeud dit “racine” qui est le tout premier noeud de l’arbre puis de deux noeuds enfants. Chaque noeud non-terminal possède à son tour deux noeuds enfants. Puis nous arrivons, après un certain nombre de feuilles, aux noeuds terminaux avec les décisions.</p>
<p>Si la condition au noeud <span class="math inline">\(k\)</span> est vérifiée alors on suit la branche de gauche, sinon on suit celle de droite.</p>
<p>Voici le début de l’arbre que nous avons obtenu.</p>
<center>
<img src="code/arbre/arbre2.png" width="600">
</center>
<p>Explicitons chaque élément :</p>
<p><span class="math inline">\(\bullet\)</span> <span class="math inline">\(x[0]&lt;1.549\)</span> : c’est la condition, si l’abscisse est inférieure à <span class="math inline">\(1.549\)</span> on va à gauche, sinon à droite</p>
<p><span class="math inline">\(\bullet\)</span> entropy=<span class="math inline">\(2\)</span> : l’entropie sur les données dans la classe (ici toutes les données) vaut <span class="math inline">\(2\)</span></p>
<p><span class="math inline">\(\bullet\)</span> samples=<span class="math inline">\(448\)</span> : il y a pour commencer <span class="math inline">\(448\)</span> données</p>
<p><span class="math inline">\(\bullet\)</span> value=<span class="math inline">\([112,112,112,112]\)</span> : il y a <span class="math inline">\(112\)</span> valeurs dans chaque classe</p>
<p>Si la condition est vérifiée je vais donc à gauche et cette fois j’ai plus que <span class="math inline">\(407\)</span> données, une entropie de <span class="math inline">\(1.999\)</span> sur ces données puis <span class="math inline">\(98\)</span> données dans la classe <span class="math inline">\(1\)</span>, <span class="math inline">\(104\)</span> dans la classe <span class="math inline">\(2\)</span>, <span class="math inline">\(107\)</span> dans la classe <span class="math inline">\(3\)</span> et <span class="math inline">\(98\)</span> dans la dernière.</p>
</section>
<section id="question-5" class="level3">
<h3 class="anchored" data-anchor-id="question-5">Question 5 <a name="Question 5" href=""></a></h3>
<p>Nous allons à présent créer <span class="math inline">\(n=160\)</span> nouvelles données que nous allons utiliser comme données de test. Nous allons calculer la proportion d’erreur faite pour les arbres précédents.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>data_test <span class="op">=</span> rand_checkers(<span class="dv">40</span>, <span class="dv">40</span>, <span class="dv">40</span>, <span class="dv">40</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> data_test[:, :<span class="dv">2</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data_test[:, <span class="dv">2</span>].astype(<span class="bu">int</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>scores_entropy <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>scores_gini <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    dt_entropy.fit(X_train, Y_train)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    scores_entropy[i] <span class="op">=</span> dt_entropy.score(X_test, Y_test)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    dt_gini.fit(X_train, Y_train)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    scores_gini[i] <span class="op">=</span> dt_gini.score(X_test, Y_test)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_entropy, label<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_gini, label<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"entropy"</span>, <span class="st">"gini"</span>])</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max depth'</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy Score'</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error with entropy and gini criterion'</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Scores with entropy criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_entropy)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>best_depth <span class="op">=</span> np.argmin(<span class="dv">1</span><span class="op">-</span>scores_entropy)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best depth for entropy criterion: "</span>, best_depth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Scores with entropy criterion:  [0.73125 0.71875 0.59375 0.48125 0.3875  0.31875 0.19375 0.16875 0.175
 0.16875 0.175   0.1875  0.1875  0.20625 0.175  ]
Best depth for entropy criterion:  8</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-8-output-2.png" width="589" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nous pouvons observer une descente plus rapide mais qui ne mène pas à une erreur nulle. En effet, plus la profondeur est élevée plus l’erreur diminue mais on remarque que l’on obtient une erreur minimale autour de <span class="math inline">\(0.19\)</span>. Ce n’est pas très étonnant car cette fois nous avons des données tests différentes des données d’entrainement et donc si on augmente la profondeur, l’erreur diminue. Dans notre exemple, la valeure de max_depth est donnée par <code>best_depth</code>et vaut <span class="math inline">\(8\)</span> pour cette graine.</p>
</section>
<section id="question-6" class="level3">
<h3 class="anchored" data-anchor-id="question-6">Question 6 <a name="Question 6" href=""></a></h3>
<p>Dans cette question nous allons utiliser le jeu de données <code>digits</code> qui est disponible dans le package <code>sklearn</code>. Le but est de recommencer l’analyse précédente mais pour un vrai jeu de données.</p>
<p>On sépare le jeu en deux : <span class="math inline">\(80%\)</span> de données d’apprentissage / <span class="math inline">\(20%\)</span> de données de tests.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the digits dataset</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># create train and test set</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(digits.data, digits.target, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>dt_entropy.fit(X_train, Y_train)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>dt_gini.fit(X_train, Y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Nous allons ensuite tracer les courbes d’erreurs pour les deux critères sur l’échantillon d’apprentissage. Nous prenons une profondeur de <span class="math inline">\(15\)</span>.</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>Error with entropy criterion:  [0.79610299 0.61934586 0.44398051 0.29645094 0.19137091 0.09881698
 0.05427975 0.0236604  0.00695894 0.00139179 0.         0.
 0.         0.         0.        ]
Error with Gini criterion:  [8.00278358e-01 6.80584551e-01 5.08002784e-01 4.00835073e-01
 2.97842728e-01 1.75365344e-01 9.53375087e-02 5.84551148e-02
 3.13152401e-02 1.46137787e-02 3.47947112e-03 6.95894224e-04
 0.00000000e+00 0.00000000e+00 0.00000000e+00]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-10-output-2.png" width="589" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Nous pouvons observer sans grande surprise que l’erreur diminue jusqu’à atteindre la valeur nulle. C’est cohérent avec ce que nous avions constaté au début.</p>
<p>Nous pouvons maintenant regarder l’erreur sur les données tests, ce qui nous intéressent le plus. Nous avons initialisé la profondeur maximum à <span class="math inline">\(20\)</span>.</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-stdout">
<pre><code>Error with entropy criterion:  [0.825      0.675      0.51666667 0.34166667 0.24444444 0.175
 0.16111111 0.13333333 0.13055556 0.14722222 0.14166667 0.13611111
 0.14444444 0.15       0.13888889 0.13055556 0.13888889 0.13055556
 0.13055556 0.15      ]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-11-output-2.png" width="589" height="376" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Pour les données tests nous obtenons une erreur qui diminue également et qui “stagne” autour de <span class="math inline">\(0.15\)</span>. On arrive à peu près à cette valeur lorsque la profondeur arrive à <span class="math inline">\(7\)</span>. Pour ce jeu de données, on voit donc qu’il n’y a pas besoin d’une grande profondeur pour obtenir des erreurs plus faibles.</p>
<p>Malheureusement, en réalité, nous n’avons pas toujours un ensemble de test à disposition.</p>
</section>
</section>
<section id="méthodes-de-choix-de-paramètres---sélection-de-modèle" class="level2">
<h2 class="anchored" data-anchor-id="méthodes-de-choix-de-paramètres---sélection-de-modèle">Méthodes de choix de paramètres - Sélection de modèle</h2>
<p><a name="Méthodes de choix de paramètres" href=""></a></p>
<p>Pour sélectionner un modèle ou un paramètre tout en considérant le plus grand nombre d’exemples possibles pour l’apprentissage, on utilise généralement une sélection par validation croisée.</p>
<section id="question-7" class="level3">
<h3 class="anchored" data-anchor-id="question-7">Question 7 <a name="Question 7" href=""></a></h3>
<p>La fonction <code>cross_val_score</code> réalise une validation croisée poour nous permettre de trouver la profondeur de l’arbre qui minimise l’erreur. cette fonction prend en entrée un arbre, selon un critère et une profondeur maximum ainsi que les observations <span class="math inline">\(X\)</span> et les réponses <span class="math inline">\(y\)</span> de nos données.</p>
<p>La fonction va choisir un ensemble d’entrainement et un ensemble de test dans <span class="math inline">\(X\)</span>, l’arbre va apprendre sur les données d’entrainement, puis tester sur les données tests et ensuite on va regarder la véracité des résultats par rapport aux réponses <span class="math inline">\(y\)</span>. <code>cross_val_score</code> réalise plusieurs fois ce schéma (argument: <code>cv=10</code>) puis nous obtenons un vecteur avec les scores.</p>
<p>Nous allons, suite à cela, faire la moyenne de ces valeurs. Ce schéma sera répété plusieurs fois en fonction de la profondeur de l’arbre.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>error_ent <span class="op">=</span> []</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>error_gini <span class="op">=</span> []</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.data</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> cross_val_score(dt_entropy, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    error_ent.append(<span class="dv">1</span><span class="op">-</span>accuracy.mean())</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>,</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                                          max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    accuracy2 <span class="op">=</span> cross_val_score(dt_gini, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    error_gini.append(<span class="dv">1</span><span class="op">-</span>accuracy2.mean())</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.plot(error_ent, label<span class="op">=</span><span class="st">"entropy"</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.plot(error_gini, label<span class="op">=</span><span class="st">"gini"</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Depth'</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Error"</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Error with entropy and gini criterion"</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_ent)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_gini)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>best_depth <span class="op">=</span> np.argmin(error_ent) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best_depth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-12-output-1.png" width="589" height="376" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.8035599006828057, 0.662250155183116, 0.5047268777157046, 0.3572284295468654, 0.26373680943513345, 0.2314711359404097, 0.19030726256983255, 0.17419615145872136, 0.181415270018622, 0.17974239602731212, 0.18865301055245176, 0.18307883302296712]
[0.8024487895716946, 0.6888857852265673, 0.5408659217877095, 0.4551862197392924, 0.3650217256362509, 0.2654562383612662, 0.20924270639354448, 0.18586902545003103, 0.17084419615145874, 0.17031967721911856, 0.17917442582247056, 0.17585661080074488]
8</code></pre>
</div>
</div>
<p>Nous obtenons une erreur qui descend jusqu’à environ <span class="math inline">\(0.2\)</span> pour <span class="math inline">\(depth=6\)</span> puis qui “stagne”. La meilleure valeur pour la profondeur est <span class="math inline">\(8\)</span> pour cette graine là. La répartition réalisée par la validation croisée est aléatoire et donc nous pouvons obtenir des résultats avec une profondeur plus faible comme plus élevé pour un même jeu de données. Cependant, comme nous avons réalisé une moyenne avec cette méthode nous aurons quand même des valeurs plutôt proches.</p>
</section>
<section id="question-8" class="level3">
<h3 class="anchored" data-anchor-id="question-8">Question 8 <a name="Question 8" href=""></a></h3>
<p>Pour cette question, nous allons tracer les courbes d’apprentissage en fonction de la taille de l’ensemble d’entrainement. Les courbes d’apprentissage nous donne une valeur du score pour des ensembles de tailles différentes. Nous tracerons les courbes avec les “intervalles de confiance”, c’est-à-dire la dispersion des scores lors de la validation croisée.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># modèle d'apprentissage</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.data</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>, max_depth<span class="op">=</span>best_depth)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Générer la courbe d'apprentissage</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>n_samples, train_scores, test_scores <span class="op">=</span> learning_curve(model, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculer les moyennes et les écarts types des scores d'entraînement</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># et de test</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>train_scores_mean <span class="op">=</span> np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>train_scores_std <span class="op">=</span> np.std(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>test_scores_mean <span class="op">=</span> np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>test_scores_std <span class="op">=</span> np.std(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Tracer la courbe d'apprentissage avec intervalle de confiance</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Learning curve"</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Training set size"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Score"</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.fill_between(n_samples, train_scores_mean <span class="op">-</span> train_scores_std,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>                 train_scores_mean <span class="op">+</span> train_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>plt.fill_between(n_samples, test_scores_mean <span class="op">-</span> test_scores_std,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>                 test_scores_mean <span class="op">+</span> test_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">"green"</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>plt.plot(n_samples, train_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">"Train score"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>plt.plot(n_samples, test_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">"Score by cross-validation"</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="TP2-arbres_de_decision_files/figure-html/cell-13-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>La courbe d’apprentissage sur les données d’entrainement vaut <span class="math inline">\(1\)</span> puis diminue légèrement plus le nombre de données augmente. Nous avons déjà vu que si nous testons nos données d’apprentissage nous apprenons très bien dans le cas où la profondeur de l’arbre est assez élevé, et ici, nous avons pris la profondeur qui minimise l’erreur c’est donc cohérent. De plus, le score diminue lorsque le nombre de données augmente car plus il y a de données d’apprentissage plus il y a de risque de sur-apprentissage et donc l’erreur n’est pas nulle.</p>
<p>Concernant la courbe d’apprentissage sur les données lors de la validation croisée, nous observons une augmentation du score lorsque le nombre de données d’entrainement augmente. En effet, plus on a de données d’entrainement, plus on apprend et donc le risque d’erreur diminue. À l’origine de la courbe nous sommes à <span class="math inline">\(200\)</span> données d’entrainement et le score est inférieure à <span class="math inline">\(0.6\)</span>, on est dans le cadre du sous-apprentissage. Il n’y a pas assez de données pour donner des résultats réellement concluant, il y a plus d’erreur commise lors de la fabrication de <code>y_test</code>.</p>
<!-- -->

</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "TP2 - arbres de décision"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">title-block-banner:</span><span class="co"> true</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:   </span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    theme: minty</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 3</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="an">toc-title:</span><span class="co"> "Sommaire"</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Anne Bernard</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2023-09-19</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="fu"># Arbre de décision - algorithme CART</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;a</span> <span class="er">name</span><span class="ot">=</span><span class="st">"Arbre de décision - algorithme CART"</span><span class="kw">&gt;&lt;/a&gt;</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification avec les arbres</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;a</span> <span class="er">name</span><span class="ot">=</span><span class="st">"Classification avec les arbres"</span><span class="kw">&gt;&lt;/a&gt;</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>Dans toute la suite, nous fixerons une graine pour pouvoir discuter des résultats plus simplement car c'est aléatoire donc nous pouvons ne pas obtenir exactement les mêmes résultats à chaque simulation.</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 1 &lt;a name="Question 1"&gt;&lt;/a&gt;</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>Dans le cadre de la régression, lorsque l'on cherche à prédire une valeur numérique pour $Y$, on peut utiliser la variance comme moyen de mesurer l'homogénéité. En effet, plus la variance entre les données est élevée plus les données sont hétérogènes, plus elle est faible et plus les données sont homogènes. </span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 2 &lt;a name="Question 2"&gt;&lt;/a&gt;</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>Dans un premier temps, créons deux arbres de décision avec les critères de classification suivants : l'indice de gini et l'entropie. Nous simulons ensuite avec <span class="in">`rand_checkers`</span> un échantillon de taille $n=456$. </span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>On partitionne ensuite en 2 sous-ensembles pour avoir un ensemble d'entrainement et un ensemble de test. </span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'./code/'</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree, datasets</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> export_graphviz</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score, train_test_split</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tp_arbres_source <span class="im">import</span> (rand_gauss, rand_bi_gauss, rand_tri_gauss,</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>                              rand_checkers, rand_clown,</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                              plot_2d, frontiere)</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: FALSE</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a><span class="co"># n = 456 = 114*4</span></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rand_checkers(n1<span class="op">=</span><span class="dv">114</span>, n2<span class="op">=</span><span class="dv">114</span>, n3<span class="op">=</span><span class="dv">114</span>, n4<span class="op">=</span><span class="dv">114</span>, sigma<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> data[:, :<span class="dv">2</span>]</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data[:, <span class="dv">2</span>].astype(<span class="bu">int</span>)</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"error with gini criterion :"</span>, <span class="dv">1</span><span class="op">-</span>dt_gini.score(X_train, Y_train))</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"error with entropy criterion :"</span>, <span class="dv">1</span><span class="op">-</span>dt_entropy.score(X_train, Y_train))</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>Nous avons regardé les erreurs pour l'indice de gini et l'entropie sur les données d'entrainements et nous obtenons 0 pour les deux. En effet, nous avons le même échantillon pour les variables à prédire que pour les observations, donc nous apprenons très bien sur cet échantillon. Nous pourrions regarder pour quelle profondeur nous obtenons $0$. Probablement que l'algorithme s'arrête lorsque l'on atteint cette valeur ($1$ en l'occurence car il raisonne en terme de score).</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>scores_entropy <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>scores_gini <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">10</span>))</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>    dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>    scores_entropy[i] <span class="op">=</span> dt_entropy.score(X_train, Y_train)</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>    dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>    scores_gini[i] <span class="op">=</span> dt_gini.score(X_train, Y_train)</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">3</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>    frontiere(<span class="kw">lambda</span> x: dt_gini.predict(x.reshape((<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))), X_train, Y_train,</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a>              step<span class="op">=</span><span class="dv">50</span>, samples<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_entropy, label<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_gini, label<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max depth'</span>)</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Error'</span>)</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error with entropy and gini criterion'</span>)</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error with entropy criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_entropy)</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error with Gini criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_gini)</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a>Lorsque l'on regarde l'erreur pour différentes profondeurs de l'arbre, on se rend compte que plus il est profond plus l'on se rapproche de $0$. Ici nous avons pris max_depth=$12$. En effet au-delà de cette valeur l'erreur est nulle ou presque. Nous pouvons le visualiser sur les courbes.</span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a>On peut noter également que si l'on prend un arbre avec une petite profondeur, l'erreur est très élevée car si les données sont bien réparties et que l'on ne peut faire qu'une coupe par exemple, nous aurons deux classes très hétérogènes.</span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a>Nous avons également afficher les frontières et nous pouvons observer une belle classification des données lorsque la profondeur est élevée. </span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 3 &lt;a name="Question 3"&gt;&lt;/a&gt;</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>Affichons la classification que l'on obtient avec la profondeur qui minimise le pourcentage d'erreurs obtenues avec l'entropie. Nous avons vu que pour depth=$12$ l'erreur est nulle (le score vaut $1$, score=1-erreur).</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>dt_entropy.max_depth <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>frontiere(<span class="kw">lambda</span> x: dt_entropy.predict(x.reshape((<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))), X_train, Y_train,</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a>          step<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Best frontier with entropy criterion"</span>)</span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best scores with entropy criterion: "</span>, dt_entropy.score(X_train,</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>                                                               Y_train))</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 4 &lt;a name="Question 4"&gt;&lt;/a&gt;</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>Grâce aux lignes suivantes, nous pouvons réaliser l'arbre de décision pour le critère d'entropie. Affichons l'arbre en question :</span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: FALSE</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>tree.plot_tree(dt_entropy)</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(dt_entropy, out_file<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>graph.render(<span class="st">"./arbre/arbre"</span>, <span class="bu">format</span><span class="op">=</span><span class="st">'pdf'</span>)</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: FALSE</span></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(dt_entropy, out_file<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>graph.render(<span class="st">"./arbre/arbre"</span>, <span class="bu">format</span><span class="op">=</span><span class="st">'png'</span>)</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">'code/arbre/arbre.png'</span> <span class="er">width</span><span class="ot">=</span><span class="st">800</span><span class="kw">&gt;</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a>Un arbre de décision est assez simple à lire. Tout d'abord il se compose d'un noeud dit "racine" qui est le tout premier noeud de l'arbre puis de deux noeuds enfants. Chaque noeud non-terminal possède à son tour deux noeuds enfants. Puis nous arrivons, après un certain nombre de feuilles, aux noeuds terminaux avec les décisions. </span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>Si la condition au noeud $k$ est vérifiée alors on suit la branche de gauche, sinon on suit celle de droite. </span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>Voici le début de l'arbre que nous avons obtenu. </span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;center&gt;&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">'code/arbre/arbre2.png'</span> <span class="er">width</span><span class="ot">=</span><span class="st">600</span><span class="kw">&gt;&lt;/center&gt;</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a>Explicitons chaque élément :</span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a>$\bullet$ $x<span class="co">[</span><span class="ot">0</span><span class="co">]</span>&lt;1.549$ : c'est la condition, si l'abscisse est inférieure à $1.549$ on va à gauche, sinon à droite</span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>$\bullet$ entropy=$2$ : l'entropie sur les données dans la classe (ici toutes les données) vaut $2$</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>$\bullet$ samples=$448$ : il y a pour commencer $448$ données</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>$\bullet$ value=$<span class="co">[</span><span class="ot">112,112,112,112</span><span class="co">]</span>$ : il y a $112$ valeurs dans chaque classe</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a>Si la condition est vérifiée je vais donc à gauche et cette fois j'ai plus que $407$ données, une entropie de $1.999$ sur ces données puis $98$ données dans la classe $1$, $104$ dans la classe $2$, $107$ dans la classe $3$ et $98$ dans la dernière. </span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 5 &lt;a name="Question 5"&gt;&lt;/a&gt;</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>Nous allons à présent créer $n=160$ nouvelles données que nous allons utiliser comme données de test. Nous allons calculer la proportion d'erreur faite pour les arbres précédents. </span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: FALSE</span></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">145</span>)</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a>data_test <span class="op">=</span> rand_checkers(<span class="dv">40</span>, <span class="dv">40</span>, <span class="dv">40</span>, <span class="dv">40</span>)</span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> data_test[:, :<span class="dv">2</span>]</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data_test[:, <span class="dv">2</span>].astype(<span class="bu">int</span>)</span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>scores_entropy <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>scores_gini <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>    dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>    scores_entropy[i] <span class="op">=</span> dt_entropy.score(X_test, Y_test)</span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a>    dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>    scores_gini[i] <span class="op">=</span> dt_gini.score(X_test, Y_test)</span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_entropy, label<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_gini, label<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"entropy"</span>, <span class="st">"gini"</span>])</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max depth'</span>)</span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy Score'</span>)</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error with entropy and gini criterion'</span>)</span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Scores with entropy criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_entropy)</span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>best_depth <span class="op">=</span> np.argmin(<span class="dv">1</span><span class="op">-</span>scores_entropy)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best depth for entropy criterion: "</span>, best_depth)</span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>Nous pouvons observer une descente plus rapide mais qui ne mène pas à une erreur nulle. En effet, plus la profondeur est élevée plus l'erreur diminue mais on remarque que l'on obtient une erreur minimale autour de $0.19$. Ce n'est pas très étonnant car cette fois nous avons des données tests différentes des données d'entrainement et donc si on augmente la profondeur, l'erreur diminue.</span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>Dans notre exemple, la valeure de max_depth est donnée par <span class="in">`best_depth`</span>et vaut $8$ pour cette graine. </span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 6 &lt;a name="Question 6"&gt;&lt;/a&gt;</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>Dans cette question nous allons utiliser le jeu de données <span class="in">`digits`</span> qui est disponible dans le package <span class="in">`sklearn`</span>. Le but est de recommencer l'analyse précédente mais pour un vrai jeu de données. </span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>On sépare le jeu en deux : $80%$ de données d'apprentissage / $20%$ de données de tests. </span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: FALSE</span></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the digits dataset</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits()</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="co"># create train and test set</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a>X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(digits.data, digits.target, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a>dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a>dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>Nous allons ensuite tracer les courbes d'erreurs pour les deux critères sur l'échantillon d'apprentissage. Nous prenons une profondeur de $15$. </span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: FALSE</span></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>scores_entropy <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>scores_gini <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>    dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>    scores_entropy[i] <span class="op">=</span> dt_entropy.score(X_train, Y_train)</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>    dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>    scores_gini[i] <span class="op">=</span> dt_gini.score(X_train, Y_train)</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_entropy, label<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_gini, label<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max depth'</span>)</span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy Score'</span>)</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"entropy"</span>, <span class="st">"gini"</span>])</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error with entropy and gini criterion'</span>)</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error with entropy criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_entropy)</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error with Gini criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_gini)</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>Nous pouvons observer sans grande surprise que l'erreur diminue jusqu'à atteindre la valeur nulle. C'est cohérent avec ce que nous avions constaté au début.</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a>Nous pouvons maintenant regarder l'erreur sur les données tests, ce qui nous intéressent le plus. Nous avons initialisé la profondeur maximum à $20$. </span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: FALSE</span></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a>scores_entropy <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>scores_gini <span class="op">=</span> np.zeros(dmax)</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a>    dt_entropy.fit(X_train, Y_train)</span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>    scores_entropy[i] <span class="op">=</span> dt_entropy.score(X_test, Y_test)</span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a>    dt_gini.fit(X_train, Y_train)</span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a>    scores_gini[i] <span class="op">=</span> dt_gini.score(X_test, Y_test)</span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_entropy, label<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="dv">1</span><span class="op">-</span>scores_gini, label<span class="op">=</span><span class="st">'gini'</span>)</span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"entropy"</span>, <span class="st">"gini"</span>])</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Max depth'</span>)</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy Score'</span>)</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Error with entropy and gini criterion'</span>)</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>plt.draw()</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Error with entropy criterion: "</span>, <span class="dv">1</span><span class="op">-</span>scores_entropy)</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>Pour les données tests nous obtenons une erreur qui diminue également et qui "stagne" autour de $0.15$. On arrive à peu près à cette valeur lorsque la profondeur arrive à $7$. Pour ce jeu de données, on voit donc qu'il n'y a pas besoin d'une grande profondeur pour obtenir des erreurs plus faibles. </span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>Malheureusement, en réalité, nous n'avons pas toujours un ensemble de test à disposition.</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="fu">## Méthodes de choix de paramètres - Sélection de modèle</span></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;a</span> <span class="er">name</span><span class="ot">=</span><span class="st">"Méthodes de choix de paramètres"</span><span class="kw">&gt;&lt;/a&gt;</span></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>Pour sélectionner un modèle ou un paramètre tout en considérant le plus grand nombre d'exemples possibles pour l'apprentissage, on utilise généralement une sélection par validation croisée. </span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 7 &lt;a name="Question 7"&gt;&lt;/a&gt;</span></span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>La fonction <span class="in">`cross_val_score`</span> réalise une validation croisée poour nous permettre de trouver la profondeur de l'arbre qui minimise l'erreur. cette fonction prend en entrée un arbre, selon un critère et une profondeur maximum ainsi que les observations $X$ et les réponses $y$ de nos données.</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>La fonction va choisir un ensemble d'entrainement et un ensemble de test dans $X$, l'arbre va apprendre sur les données d'entrainement, puis tester sur les données tests et ensuite on va regarder la véracité des résultats par rapport aux réponses $y$. <span class="in">`cross_val_score`</span> réalise plusieurs fois ce schéma (argument: <span class="in">`cv=10`</span>) puis nous obtenons un vecteur avec les scores. </span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>Nous allons, suite à cela, faire la moyenne de ces valeurs. Ce schéma sera répété plusieurs fois en fonction de la profondeur de l'arbre.</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>error_ent <span class="op">=</span> []</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>error_gini <span class="op">=</span> []</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>dmax <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.data</span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dmax):</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>    dt_entropy <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>                                             max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> cross_val_score(dt_entropy, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a>    error_ent.append(<span class="dv">1</span><span class="op">-</span>accuracy.mean())</span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>    dt_gini <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'gini'</span>,</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>                                          max_depth<span class="op">=</span>i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>    accuracy2 <span class="op">=</span> cross_val_score(dt_gini, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a>    error_gini.append(<span class="dv">1</span><span class="op">-</span>accuracy2.mean())</span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">4</span>))</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>plt.plot(error_ent, label<span class="op">=</span><span class="st">"entropy"</span>)</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>plt.plot(error_gini, label<span class="op">=</span><span class="st">"gini"</span>)</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Depth'</span>)</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Error"</span>)</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Error with entropy and gini criterion"</span>)</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_ent)</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(error_gini)</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a>best_depth <span class="op">=</span> np.argmin(error_ent) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best_depth)</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>Nous obtenons une erreur qui descend jusqu'à environ $0.2$ pour $depth=6$ puis qui "stagne". La meilleure valeur pour la profondeur est $8$ pour cette graine là. La répartition réalisée par la validation croisée est aléatoire et donc nous pouvons obtenir des résultats avec une profondeur plus faible comme plus élevé pour un même jeu de données. Cependant, comme nous avons réalisé une moyenne avec cette méthode nous aurons quand même des valeurs plutôt proches.  </span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a><span class="fu">### Question 8 &lt;a name="Question 8"&gt;&lt;/a&gt;</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a>Pour cette question, nous allons tracer les courbes d'apprentissage en fonction de la taille de l'ensemble d'entrainement. Les courbes d'apprentissage nous donne une valeur du score pour des ensembles de tailles différentes. Nous tracerons les courbes avec les "intervalles de confiance", c'est-à-dire la dispersion des scores lors de la validation croisée. </span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">12</span>)</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a><span class="co"># modèle d'apprentissage</span></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.data</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>, max_depth<span class="op">=</span>best_depth)</span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="co"># Générer la courbe d'apprentissage</span></span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a>n_samples, train_scores, test_scores <span class="op">=</span> learning_curve(model, X, y, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculer les moyennes et les écarts types des scores d'entraînement</span></span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a><span class="co"># et de test</span></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>train_scores_mean <span class="op">=</span> np.mean(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a>train_scores_std <span class="op">=</span> np.std(train_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>test_scores_mean <span class="op">=</span> np.mean(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a>test_scores_std <span class="op">=</span> np.std(test_scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a><span class="co"># Tracer la courbe d'apprentissage avec intervalle de confiance</span></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Learning curve"</span>)</span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Training set size"</span>)</span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Score"</span>)</span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>plt.fill_between(n_samples, train_scores_mean <span class="op">-</span> train_scores_std,</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a>                 train_scores_mean <span class="op">+</span> train_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a>plt.fill_between(n_samples, test_scores_mean <span class="op">-</span> test_scores_std,</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>                 test_scores_mean <span class="op">+</span> test_scores_std, alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">"green"</span>)</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>plt.plot(n_samples, train_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">"Train score"</span>)</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a>plt.plot(n_samples, test_scores_mean, <span class="st">'o-'</span>, color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="st">"Score by cross-validation"</span>)</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>La courbe d'apprentissage sur les données d'entrainement vaut $1$ puis diminue légèrement plus le nombre de données augmente. Nous avons déjà vu que si nous testons nos données d'apprentissage nous apprenons très bien dans le cas où la profondeur de l'arbre est assez élevé, et ici, nous avons pris la profondeur qui minimise l'erreur c'est donc cohérent. De plus, le score diminue lorsque le nombre de données augmente car plus il y a de données d'apprentissage plus il y a de risque de sur-apprentissage et donc l'erreur n'est pas nulle.</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a>Concernant la courbe d'apprentissage sur les données lors de la validation croisée, nous observons une augmentation du score lorsque le nombre de données d'entrainement augmente. En effet, plus on a de données d'entrainement, plus on apprend et donc le risque d'erreur diminue. À l'origine de la courbe nous sommes à $200$ données d'entrainement et le score est inférieure à $0.6$, on est dans le cadre du sous-apprentissage. Il n'y a pas assez de données pour donner des résultats réellement concluant, il y a plus d'erreur commise lors de la fabrication de <span class="in">`y_test`</span>. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>